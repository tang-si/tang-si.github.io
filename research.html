
<!doctype html>
<html lang="en">
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Nian (Oakley) Liu</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- New GA4 tracking code, see https://support.google.com/analytics/answer/10271001#analyticsjs-enable-basic --> 
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-GNJD50R0Z7"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-GNJD50R0Z7');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
      .social-row {
        display: flex;
        flex-wrap: wrap;
        justify-content: space-between;
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Nian (Oakley) Liu</h1>
        <p>Master of Computer Science<br>Beijing University of Posts and Telecommunications</p>
    <h3><a href="https://liun-online.github.io/">Home</a></h3>
    <h3><a href="https://liun-online.github.io/research/CVs.pdf">CV</a></h3>  
        <h3><a href="https://liun-online.github.io/research.html">Publications</a></h3>
        <h3><a href="https://liun-online.github.io/personal.html">Personal</a></h3>
    <b>Social</b><br>
        <div class="social-row">
          <a href="mailto:oakley.liun@gmail.com" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a><br>
          <a href="https://scholar.google.com/citations?user=Tx8vRjUAAAAJ&hl=en" target="_blank"><i class="ai ai-fw ai-google-scholar-square"></i> Scholar</a><br>
          <a href="https://github.com/liun-online"><i class="fa fa-fw fa-github-square"></i> GitHub</a><br>
          <br>
        </div>
        <br>

    <p><b>Contact:</b><br>Department of Computer Science<br>Beijing University of Posts and Telecommunications<br>Xitucheng Road 10<br>Beijing haidian district, China</p>
    <p><b>Post</b>: 100876</p>
    <p><b>Phone</b>: +86 13041092011</p>
      </header>
      <section>
    <h2><a id="published-papers-updated" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Published Papers</h2>
    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://arxiv.org/abs/2210.02330">Revisiting Graph Contrastive Learning from the Perspective of Graph Spectrum</a> <br> <b>Authors</b>: <b>Nian Liu</b>, Xiao Wang, Deyu Bo, Chuan Shi, Jian Pei<br> <b>Conference</b>: <i>Advances in neural information processing systems</i> <b>(NeurIPS)</b>, 2022<br>
    <p style="margin:0"><img src="./research/model_spco.PNG" height="200" width="500"></p>
    <p style="margin:0"> <b>Overview</b>: This is the first attempt to fundamentally explore the augmentation strategies for GCL from spectral 
domain. We not only reveal the general graph augmentation rule behind different augmentation, but also 
explain why GCL works by proposing the contrastive invariance theorem. Our work provides deeper 
understanding on the nature of GCL.</p> 
    <p style="margin:0"> <b>Code</b>: https://github.com/liun-online/SpCo</p> 
    <p style="margin:0"><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>
Graph Contrastive Learning (GCL), learning the node representations by augmenting graphs, has attracted considerable attentions. Despite the proliferation of
various graph augmentation strategies, some fundamental questions still remain
unclear: what information is essentially encoded into the learned representations
by GCL? Are there some general graph augmentation rules behind different augmentations? If so, what are they and what insights can they bring? In this paper,
we answer these questions by establishing the connection between GCL and graph
spectrum. By an experimental investigation in spectral domain, we firstly find
the General grAph augMEntation (GAME) rule for GCL, i.e., the difference of
the high-frequency parts between two augmented graphs should be larger than
that of low-frequency parts. This rule reveals the fundamental principle to revisit
the current graph augmentations and design new effective graph augmentations.
Then we theoretically prove that GCL is able to learn the invariance information
by contrastive invariance theorem, together with our GAME rule, for the first
time, we uncover that the learned representations by GCL essentially encode the
low-frequency information, which explains why GCL works. Guided by this
rule, we propose a spectral graph contrastive learning module (SpCo1
), which is
a general and GCL-friendly plug-in. We combine it with different existing GCL
models, and extensive experiments well demonstrate that it can further improve the
performances of a wide variety of different GCL methods.</p></div>
    <p style="margin:0"><button class="accordion">
      Poster
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Update soon </p></div>
    <p style="margin:0"><button class="accordion">
      Vedio
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Update soon </p></div><br>
    
    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://arxiv.org/abs/2201.05540">Compact Graph Structure Learning via Mutual Information Compression</a> <br> <b>Authors</b>: <b>Nian Liu</b>, Xiao Wang, Lingfei Wu, Yu Chen, Xiaojie Guo, Chuan Shi<br> <b>Conference</b>: <i>International World Wide Web Conference</i> <b>(TheWebConf)</b>, 2022 <br>
    <p style="margin:0"><img src="./research/model_cogsl.png" height="200" width="500"></p>
    <p style="margin:0"> <b>Overview</b>:  In this paper, we are the first to define the “optimal graph structure” in principle by Information 
theory, which can achieve effectiveness and robustness simultaneously.</p> 
    <p style="margin:0"> <b>Code</b>: https://github.com/liun-online/CoGSL</p> 
    <p style="margin:0"><button class="accordion"> 
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Graph Structure Learning (GSL) recently has attracted considerable
attentions in its capacity of optimizing graph structure as well as
learning suitable parameters of Graph Neural Networks (GNNs) simultaneously. Current GSL methods mainly learn an optimal graph
structure (final view) from single or multiple information sources
(basic views), however the theoretical guidance on what is the optimal graph structure is still unexplored. In essence, an optimal graph
structure should only contain the information about tasks while
compress redundant noise as much as possible, which is defined as
"minimal sufficient structure", so as to maintain the accurancy and
robustness. How to obtain such structure in a principled way? In
this paper, we theoretically prove that if we optimize basic views
and final view based on mutual information, and keep their performance on labels simultaneously, the final view will be a minimal
sufficient structure. With this guidance, we propose a Compact GSL
architecture by MI compression, named CoGSL. Specifically, two
basic views are extracted from original graph as two inputs of the
model, which are refinedly reestimated by a view estimator. Then,
we propose an adaptive technique to fuse estimated views into the
final view. Furthermore, we maintain the performance of estimated
views and the final view and reduce the mutual information of every
two views. To comprehensively evaluate the performance of CoGSL,
we conduct extensive experiments on several datasets under clean
and attacked conditions, which demonstrate the effectiveness and
robustness of CoGSL. </p></div>
    <p style="margin:0"><button class="accordion">
      Vedio
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://www.youtube.com/watch?v=cSohAhgO8Po">Online Presentation</a> </p></div><br>
    
    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://arxiv.org/abs/2105.09111">Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning</a> <br> <b>Authors</b>: Xiao Wang, <b>Nian Liu</b>, Hui Han, Chuan Shi <br><b>Conference</b>: <i>Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining</i> <b>(KDD)</b>, 2021<br>
    <p style="margin:0"><img src="./research/model_heco.png" height="200" width="500"></p>
    <p style="margin:0"> <b>Overview</b>:  HeCo is the first to conduct cross-view contrastive learning in heterogeneous graph. According to 
PaperDigest, this paper is one of the most influential paper in KDD 2021 as for 2022/05.</p> 
    <p style="margin:0"> <b>Code</b>: https://github.com/liun-online/HeCo</p> 
    <p style="margin:0"><button class="accordion"> 
    Abstract
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Heterogeneous graph neural networks (HGNNs) as an emerging technique have shown superior capacity of dealing with heterogeneous information network (HIN). However, most HGNNs follow a semi-supervised learning manner, which notably limits their wide use in reality since labels are usually scarce in real applications. Recently, contrastive learning, a self-supervised method, becomes one of the most exciting learning paradigms and shows great potential when there are no labels. In this paper, we study the problem of self-supervised HGNNs and propose a novel co-contrastive learning mechanism for HGNNs, named HeCo. Different from traditional contrastive learning which only focuses on contrasting positive and negative samples, HeCo employs cross-viewcontrastive mechanism. Specifically, two views of a HIN (network schema and meta-path views) are proposed to learn node embeddings, so as to capture both of local and high-order structures simultaneously. Then the cross-view contrastive learning, as well as a view mask mechanism, is proposed, which is able to extract the positive and negative embeddings from two views. This enables the two views to collaboratively supervise each other and finally learn high-level node embeddings. Moreover, two extensions of HeCo are designed to generate harder negative samples with high quality, which further boosts the performance of HeCo. Extensive experiments conducted on a variety of real-world networks show the superior performance of the proposed methods over the state-of-the-arts.</p></div>
    <p style="margin:0"><button class="accordion">
      Poster
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="./research/poster_heco.pdf">Poster</a></p></div>
    <p style="margin:0"><button class="accordion">
      Vedio
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://dl.acm.org/doi/10.1145/3447548.3467415">Online Presentation</a></p></div><br>
    
    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://dl.acm.org/doi/10.1145/3468674">Embedding Heterogeneous Information Network in Hyperbolic Spaces</a> <br> <b>Authors</b>: Yiding Zhang, Xiao Wang, <b>Nian Liu</b>, Chuan Shi <br><b>Journal</b>: <i>ACM Transactions on Knowledge Discovery from Data</i> <b>(ACM TKDD)</b>, 2021<br>
    <p style="margin:0"> <b>Overview</b>:  We are the first to explore HIN embedding in hyperbolic spaces, naturally capture the hierarchical 
and power-law structure in complex network.</p> 
    <p style="margin:0"><button class="accordion"> 
    Abstract
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Heterogeneous information network (HIN) embedding, aiming to project HIN into a low-dimensional space, has attracted considerable research attention. Most of the existing HIN embedding methods focus on preserving the inherent network structure and semantic correlations in Euclidean spaces. However, one fundamental problem is whether the Euclidean spaces are the intrinsic spaces of HIN? Recent researches find the complex network with hyperbolic geometry can naturally reflect some properties, e.g., hierarchical and power-law structure. In this article, we make an effort toward embedding HIN in hyperbolic spaces. We analyze the structures of three HINs and discover some properties, e.g., the power-law distribution, also exist in HINs. Therefore, we propose a novel HIN embedding model HHNE. Specifically, to capture the structure and semantic relations between nodes, HHNE employs the meta-path guided random walk to sample the sequences for each node. Then HHNE exploits the hyperbolic distance as the proximity measurement. We also derive an effective optimization strategy to update the hyperbolic embeddings iteratively. Since HHNE optimizes different relations in a single space, we further propose the extended model HHNE++. HHNE++ models different relations in different spaces, which enables it to learn complex interactions in HINs. The optimization strategy of HHNE++ is also derived to update the parameters of HHNE++ in a principle manner. The experimental results demonstrate the effectiveness of our proposed models.</p></div><br>
    
    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://arxiv.org/abs/2104.07477">Lorentzian Graph Convolutional Networks</a> <br> <b>Authors</b>: Yiding Zhang, Xiao Wang, Chuan Shi, <b>Nian Liu</b>, Guojie Song <br><b>Conference</b>: <i>International World Wide Web Conference</i> <b>(TheWebConf)</b>, 2021<br>
    <p style="margin:0"><img src="./research/model_lgcn.PNG" height="150" width="500"></p>
    <p style="margin:0"> <b>Overview</b>: We study on the hyperbolic GCN, rebuild graph operations with Lorentzian version and design a 
neighborhood aggregation method based on the centroid of Lorentzian distance.</p> 
    <p style="margin:0"> <b>Code</b>: https://github.com/BUPT-GAMMA/lgcn_torch</p> 
    <p style="margin:0"><button class="accordion"> 
    Abstract
    </button> 
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Graph convolutional networks (GCNs) have received considerable research attention recently. Most GCNs learn the node representations in Euclidean geometry, but that could have a high distortion in the case of embedding graphs with scale-free or hierarchical structure. Recently, some GCNs are proposed to deal with this problem in non-Euclidean geometry, e.g., hyperbolic geometry. Although hyperbolic GCNs achieve promising performance, existing hyperbolic graph operations actually cannot rigorously follow the hyperbolic geometry, which may limit the ability of hyperbolic geometry and thus hurt the performance of hyperbolic GCNs. In this paper, we propose a novel hyperbolic GCN named Lorentzian graph convolutional network (LGCN), which rigorously guarantees the learned node features follow the hyperbolic geometry. Specifically, we rebuild the graph operations of hyperbolic GCNs with Lorentzian version, e.g., the feature transformation and non-linear activation. Also, an elegant neighborhood aggregation method is designed based on the centroid of Lorentzian distance. Moreover, we prove some proposed graph operations are equivalent in different types of hyperbolic geometry, which fundamentally indicates their correctness. Experiments on six datasets show that LGCN performs better than the state-of-the-art methods. LGCN has lower distortion to learn the representation of tree-likeness graphs compared with existing hyperbolic GCNs. We also find that the performance of some hyperbolic GCNs can be improved by simply replacing the graph operations with those we defined in this paper.</p></div><br>

    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://arxiv.org/abs/2201.07708">Debiased Graph Neural Networks with Agnostic Label Selection Bias</a> <br> <b>Authors</b>: Shaohua Fan, Xiao Wang, Chuan Shi, Kun Kuang, <b>Nian Liu</b>, Bai Wang <br><b>Journal</b>: <i>IEEE Transactions on Neural Networks and Learning Systems </i> <b>(IEEE TNNLS)</b>, 2022<br>
    <p style="margin:0"><img src="./research/model_tnnls.PNG" height="200" width="500"></p>
    <p style="margin:0"> <b>Overview</b>: We focus on the label selection bias in GNN, and propose Debiased GNN to eliminate spurious 
correlation by reweighting samples and improve stability of prediction.</p> 
    <p style="margin:0"> <b>Code</b>: https://github.com/googlebaba/TNNLS2022-DGNN</p> 
    <p style="margin:0"><button class="accordion"> 
    Abstract
    </button> 
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Most existing Graph Neural Networks (GNNs) are proposed without considering the selection bias in data, i.e., the inconsistent distribution between the training set with test set. In reality, the test data is not even available during the training process, making selection bias agnostic. Training GNNs with biased selected nodes leads to significant parameter estimation bias and greatly impacts the generalization ability on test nodes. In this paper, we first present an experimental investigation, which clearly shows that the selection bias drastically hinders the generalization ability of GNNs, and theoretically prove that the selection bias will cause the biased estimation on GNN parameters. Then to remove the bias in GNN estimation, we propose a novel Debiased Graph Neural Networks (DGNN) with a differentiated decorrelation regularizer. The differentiated decorrelation regularizer estimates a sample weight for each labeled node such that the spurious correlation of learned embeddings could be eliminated. We analyze the regularizer in causal view and it motivates us to differentiate the weights of the variables based on their contribution on the confounding bias. Then, these sample weights are used for reweighting GNNs to eliminate the estimation bias, thus help to improve the stability of prediction on unknown test nodes. Comprehensive experiments are conducted on several challenging graph datasets with two kinds of label selection biases. The results well verify that our proposed model outperforms the state-of-the-art methods and DGNN is a flexible framework to enhance existing GNNs.</p></div><br>
    
    <hr>
    
    
    <h2><a id="recent-RRs-updated" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Papers under Revision</h2>
    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold">Hierarchical Contrastive Learning Enhanced Heterogeneous Graph Neural Network</a> <br> <b>Authors</b>: <b>Nian Liu</b>, Xiao Wang, Hui Han, Chuan Shi <br> <b>Journal</b>: <i>IEEE Transactions on Knowledge and Data Engineering</i> <b>(TKDE)</b><br>
    <p style="margin:0"> <b>Overview</b>: This work argues that besides cross-view contrast to capture commonality in HeCo, view-specific 
information also should be explored by intra-view contrast. </p>
    <p style="margin:0"><button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Heterogeneous graph neural networks (HGNNs) as an emerging technique have shown superior capacity of dealing with
heterogeneous information network (HIN). However, most HGNNs follow a semi-supervised learning manner, which notably limits their
wide use in reality since labels are usually scarce in real applications. Recently, contrastive learning, a self-supervised method, becomes
one of the most exciting learning paradigms and shows great potential when there are no labels. In this paper, we study the problem
of self-supervised HGNNs and propose a novel co-contrastive learning mechanism for HGNNs, named HeCo. Different from traditional
contrastive learning which only focuses on contrasting positive and negative samples, HeCo employs cross-view contrastive mechanism.
Specifically, two views of a HIN (network schema and meta-path views) are proposed to learn node embeddings, so as to capture both
of local and high-order structures simultaneously. Then the cross-view contrastive learning, as well as a view mask mechanism, is
proposed, which is able to extract the positive and negative embeddings from two views. This enables the two views to collaboratively
supervise each other and finally learn high-level node embeddings. Moreover, to further boost the performance of HeCo, two additional
methods are designed to generate harder negative samples with high quality. The essence of HeCo is to make positive samples from
different views close to each other by cross-view contrast, and learn the factors invariant to two proposed views. However, besides
the invariant factors, view-specific factors complementally provide the diverse structure information between different nodes, which also
should be contained into the final embeddings. Therefore, we need to further explore each view independently and propose a modified
model, called HeCo++. Specifically, HeCo++ conducts hierarchical contrastive learning, including cross-view and intra-view contrasts,
which aims to enhance the mining of respective structures. Extensive experiments conducted on a variety of real-world networks show
the superior performance of the proposed methods over the state-of-the-arts. </p></div>

      </section>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
